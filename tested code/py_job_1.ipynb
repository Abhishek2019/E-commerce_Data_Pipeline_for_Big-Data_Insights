{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "eddb6a31-fa0e-4e8b-abde-3122e03e7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# import sys\n",
    "# from random import random\n",
    "# from operator import add\n",
    "\n",
    "# spark = SparkSession.builder.master(\"spark://10.1.174.144:7077\").appName(\"PyCount Count\").getOrCreate()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"mongodbtest1\") \\\n",
    ".master(\"spark://10.1.174.144:7077\")\\\n",
    ".config(\"spark.mongodb.read.uri\", \"mongodb://abhi_1:60000/shardDB.asofashion_2\") \\\n",
    ".config(\"spark.mongodb.write.uri\", \"mongodb://abhi_1:60000/shardDB.asofashion_clean\") \\\n",
    ".config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.13:10.2.2') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ede21510-30ce-4aea-a84b-b70a0ccdbc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "016700df-5ccc-4708-9b22-0fa1491c8fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.1.174.144:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>mongodbtest1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f31bcdfe980>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e8310c0-7fc6-467f-bfdc-e8625dcb13bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = spark.read\\\n",
    "                 .format(\"mongodb\")\\\n",
    "                 .option(\"connection.uri\", \"mongodb://abhi_1:60000/\")\\\n",
    "                 .option(\"database\", \"shardDB\")\\\n",
    "                 .option(\"collection\", \"asofashion_2\")\\\n",
    "                 .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b126e6bf-4542-4fdc-8f92-683118396bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "90309da0-1e9f-42ce-a98e-786b2874ca26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_id='660f3b689c0dd3afddadd33c', brand_name='Brave Soul', colour=nan, currency='USD', current_price=23.0, gender='male', previous_price=nan, productCode=119545541, productType='Product', product_id=203081399, rrp=42.0, title='Brave Soul cotton check shirt in off white & brown')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0c410928-faec-4261-8889-8b44a279f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def ser(x):\n",
    "    x_dict = x.asDict()\n",
    "    x_dict[\"title\"] = x_dict[\"title\"].lower()+ \"_abcd\"\n",
    "    return Row(x_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "65cb401f-74b7-4316-b741-6301d5b1dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = dataFrame.rdd.map(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7defbcd4-f17c-4d4b-8d38-194f3ade4952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_id='660f3b689c0dd3afddadd33d', brand_name='adidas Originals', colour=nan, currency='USD', current_price=54.72, gender='male', previous_price=99.48, productCode=114983404, productType='Product', product_id=201828775, rrp=nan, title=\"adidas originals 'sports resort' three stripe wide leg track pants in wonder white_abcd\")]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "06157413-0d2b-4631-99f5-e1b881bc7b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6375ae2-1138-4c10-b75d-ee2e7eb03aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a1be9a96-5d3d-450c-9e6c-918e4b37881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d79dcd43-e812-45d7-8fcf-b0d0262fbba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pyspark.sql.types.Row]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95ed11-8f8b-4bf3-9498-97c066143171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35061c-85dc-43ab-9685-6796359f8b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "140b447d-6499-4a43-a17c-abba950e1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# def ser(x):\n",
    "#     x_dict = x.asDict()\n",
    "#     x_dict[\"title\"] = x_dict[\"title\"].lower()+ \"_abcd\"\n",
    "#     print(x_dict[\"title\"])\n",
    "\n",
    "def ser(x):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "88efa93d-8df8-46b5-96dc-6f69989d645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/10 20:16:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/10 20:16:32 WARN CaseInsensitiveStringMap: Converting duplicated key checkpointLocation into CaseInsensitiveStringMap.\n",
      "24/04/10 20:16:32 WARN CaseInsensitiveStringMap: Converting duplicated key forcedeletetempcheckpointlocation into CaseInsensitiveStringMap.\n",
      "24/04/10 20:16:32 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 16) (10.1.174.107 executor 0): java.lang.NoSuchMethodError: 'scala.collection.immutable.Seq org.apache.spark.sql.types.StructType.toAttributes()'\n",
      "\tat com.mongodb.spark.sql.connector.schema.InternalRowToRowFunction.<init>(InternalRowToRowFunction.java:46)\n",
      "\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.createInternalRowToRowFunction(RowToBsonDocumentConverter.java:311)\n",
      "\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.<init>(RowToBsonDocumentConverter.java:89)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.<init>(MongoDataWriter.java:74)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriterFactory.createWriter(MongoDataWriterFactory.java:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.MicroBatchWriterFactory.createWriter(MicroBatchWrite.scala:48)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:407)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/04/10 20:16:32 ERROR TaskSetManager: Task 0 in stage 4.0 failed 4 times; aborting job\n",
      "24/04/10 20:16:32 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@78034723 is aborting.\n",
      "24/04/10 20:16:32 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@78034723 failed to abort.\n",
      "24/04/10 20:16:32 ERROR MicroBatchExecution: Query [id = 1ca900f0-90cf-4f5d-9f0b-9666012ec9a5, runId = f11b8f01-559b-4c81-866f-0bf5e7117115] terminated with error\n",
      "org.apache.spark.SparkException: Writing job failed.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobFailedError(QueryExecutionErrors.scala:606)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:381)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2971)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:2971)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:603)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 19) (10.1.174.107 executor 0): java.lang.NoSuchMethodError: 'scala.collection.immutable.Seq org.apache.spark.sql.types.StructType.toAttributes()'\n",
      "\tat com.mongodb.spark.sql.connector.schema.InternalRowToRowFunction.<init>(InternalRowToRowFunction.java:46)\n",
      "\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.createInternalRowToRowFunction(RowToBsonDocumentConverter.java:311)\n",
      "\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.<init>(RowToBsonDocumentConverter.java:89)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.<init>(MongoDataWriter.java:74)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriterFactory.createWriter(MongoDataWriterFactory.java:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.MicroBatchWriterFactory.createWriter(MicroBatchWrite.scala:48)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:407)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:354)\n",
      "\t... 40 more\n",
      "\tSuppressed: com.mongodb.spark.sql.connector.exceptions.DataException: Write aborted for: 1ca900f0-90cf-4f5d-9f0b-9666012ec9a5. 0/1 tasks completed. EpochId: 0\n",
      "\t\tat com.mongodb.spark.sql.connector.write.MongoStreamingWrite.abort(MongoStreamingWrite.java:94)\n",
      "\t\tat org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.abort(MicroBatchWrite.scala:36)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:376)\n",
      "\t\t... 40 more\n",
      "Caused by: java.lang.NoSuchMethodError: 'scala.collection.immutable.Seq org.apache.spark.sql.types.StructType.toAttributes()'\n",
      "\tat com.mongodb.spark.sql.connector.schema.InternalRowToRowFunction.<init>(InternalRowToRowFunction.java:46)\n",
      "\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.createInternalRowToRowFunction(RowToBsonDocumentConverter.java:311)\n",
      "\tat com.mongodb.spark.sql.connector.schema.RowToBsonDocumentConverter.<init>(RowToBsonDocumentConverter.java:89)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriter.<init>(MongoDataWriter.java:74)\n",
      "\tat com.mongodb.spark.sql.connector.write.MongoDataWriterFactory.createWriter(MongoDataWriterFactory.java:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.MicroBatchWriterFactory.createWriter(MicroBatchWrite.scala:48)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:407)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "stream_query = (spark.readStream\n",
    "  .format(\"mongodb\")\n",
    "  .option(\"spark.mongodb.connection.uri\", \"mongodb://abhi_1:60000/\")\n",
    "  .option('spark.mongodb.database', \"shardDB\")\n",
    "  .option('spark.mongodb.collection', \"asofashion_2\")\n",
    "  .option(\"change.stream.publish.full.document.only\",\"true\")\\\n",
    "  .load()\n",
    "  .writeStream\n",
    "    .format(\"mongodb\")\n",
    "    .option(\"checkpointLocation\", \"./stream_meta/\")\n",
    "    .option(\"forceDeleteTempCheckpointLocation\", \"true\")\n",
    "    .option(\"connection.uri\", \"mongodb://abhi_1:60000/\")\n",
    "    .option(\"database\", \"shardDB\")\n",
    "    .option(\"collection\", \"asofashion_2\")\n",
    "    .outputMode(\"append\")\n",
    ")\n",
    "\n",
    "y = stream_query.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "60d12b96-06d9-4e1f-8d79-2f9bbb9f949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read_stream.select(\"title\").collect()\n",
    "\n",
    "# type(read_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "80ae8364-7977-4d72-bff0-c76ad7b5c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_stream.writeStream\\\n",
    "# .format(\"memory\")\\\n",
    "# .option(\"forceDeleteTempCheckpointLocation\",\"true\")\\\n",
    "# .outputMode(\"append\")\\\n",
    "# .queryName(\"abc\")\\\n",
    "# .option(\"path\",\"./check_path/\")\\\n",
    "# .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "440d8844-c0cd-468e-9a1f-81d548a90796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_stream.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e7b91-9802-4a6d-8e78-400f08dc27ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
